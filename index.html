<!doctype html>
<html lang="en">
    <head>
        <!-- META -->
        <meta charset="utf-8">
        
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
       
        <!-- PAGE TITLE -->
        <title>Xingdi Zhang - homepage</title>
       
        <!-- FAVICON -->
        <link rel="shortcut icon" href="assets/img/favicon.png">
       
        <!-- FONTS -->
        <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&amp;subset=latin-ext" rel="stylesheet">
       
     
        
    </head>
    
         <div class="Intro">
              <h1 Xingdi Zhang.</h1>
              <h2>About Me</h2>

                        <p I graduated from UESTC (CHINA), where I majored in CS. Now I am a PHD student at KAUST under the supervision of professor Markus Hadwiger</p>
                        <p >My research areas include:  parallel computing, computer vision, computer graphics, visualization</p>
        </div>



        <div class="Publications">
              <li>
                   <p>  VCIP2018: This work studies the efficient Synthesis scheme of HDR pictures, we propose a method to quickly produce synthetic weights according to the intermediate products of JPEG image compression.<br>
                       Publication of research paper <a href="https://www.researchgate.net/publication/332676147_Multi-exposure_Fusion_With_JPEG_Compression_Guidance">(Multi-exposure Fusion with JPEG Compression Guidance)</a> as first author was accepted by IEEE VCIP2018. </p>
              </li>
               <li>
                   <p>  CVPR2019:We focus on a deep learning solution for deep detection complement in on-board scenarios, we use virtual engine (UE4) to generate synthetic training data, train our model with our own multichannel multi-scale fusion depth module.<br>
                       Research paper <a href="https://arxiv.org/abs/1812.00488v1">（DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image）</a> was accepted by CVPR2019.</p>
              </li>
              <li>
                   <p>  IEEE VIS2021: This paper presents a novel framework for the exploration and use of an interactively-chosen set of physically realizable observers, to explore the volecity fields(vector fileds), to visualize objective vortex structures.<br>
                       Research paper <a href="http://vccvisualization.org/research/killinginteraction/">（Interactive Exploration of Physically-Observable Objective Vortices in Unsteady 2D Flow ）</a> was accepted by IEEE VIS2021. </p>
              </li>

        </div>
    
          
       <!-- CONTACT -->
        <section id="contact">


            <h3 class="headline scroll-animated">Contact Me</h3>

            <p> Emali: cindyzhang.yono531@gmail.com  <br> XINGDI.ZHANG@KAUST.EDU.SA</p>


        </section>
        <!-- /CONTACT -->
                
              
        <!-- FOOTER -->
        <section id="footer">
            <p class="scroll-animated">© 2017 xingdi.zhang | xxxxxx</p>
        </section>
        <!-- /FOOTER -->

   
    </body> 
    
    
</html>
