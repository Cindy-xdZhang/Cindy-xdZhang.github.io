<!doctype html>
<html lang="en">
    <head>
        <!-- META -->
        <meta charset="utf-8">
        
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
       
        <!-- PAGE TITLE -->
        <title>Xingdi Zhang - homepage</title>
       
        <!-- FAVICON -->
        <link rel="shortcut icon" href="assets/img/favicon.png">
       
        <!-- FONTS -->
        <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&amp;subset=latin-ext" rel="stylesheet">
       
     
        
    </head>
    
         <div class="Intro">
              <h1>Xingdi Zhang (张星迪)</h1>
					<img src="assets//img//IMG_20200906_115229.jpg" width="350px" height="280px">
                    <div class='CV'>
                        <h3> Education </h3>
                        
                        <p> I got my B.E of Computer science from UESTC (CHINA).<br>
                            Now I am a CS PHD student at KAUST under the supervision of professor Markus Hadwiger.<br>
                        </p>
                        
                      </div>
                         <p >My research areas include:  parallel computing, computer vision, computer graphics, visualization</p>
         </div>



        <div class="Publications">
              <li>
                   VCIP2018: This work studies the efficient Synthesis scheme of HDR pictures, we propose a method to quickly produce synthetic weights according to the intermediate products of JPEG image compression.<br>
                    Research paper <a href="https://www.researchgate.net/publication/332676147_Multi-exposure_Fusion_With_JPEG_Compression_Guidance">(Multi-exposure Fusion with JPEG Compression Guidance)</a> was accepted by IEEE VCIP2018.
              </li>
               <li>
                   CVPR2019:We focus on a deep learning solution for deep detection complement in on-board scenarios, we use virtual engine (UE4) to generate synthetic training data, train our model with our own multichannel multi-scale fusion depth module.<br>
                       Research paper <a href="https://arxiv.org/abs/1812.00488v1">（DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image）</a> was accepted by CVPR2019.
              </li>
              <li>
                   IEEE VIS2021: This paper presents a novel framework for the exploration and use of an interactively-chosen set of physically realizable observers, to explore the volecity fields(vector fileds), to visualize objective vortex structures.<br>
                       Research paper <a href="http://vccvisualization.org/research/killinginteraction/">（Interactive Exploration of Physically-Observable Objective Vortices in Unsteady 2D Flow ）</a> was accepted by IEEE VIS2021.
              </li>

        </div>
    
          
       <!-- CONTACT -->
        <section id="contact">


            <h3 class="headline scroll-animated">Contact Me</h3>

            <p> Emali: cindyzhang.yono531@gmail.com  <br> XINGDI.ZHANG@KAUST.EDU.SA</p>


        </section>
        <!-- /CONTACT -->
                
              
        <!-- FOOTER -->
        <section id="footer">
		<img src="assets//img//TIM图片20200731173705.jpg" width="350px" height="280px">
            <p class="scroll-animated">© 2017 xingdi.zhang | xxxxxx</p>
        </section>
        <!-- /FOOTER -->

   
    </body> 
    
    
</html>
